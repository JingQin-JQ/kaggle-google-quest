{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.parse import urlparse\n",
    "%matplotlib inline\n",
    "import re\n",
    "import spacy\n",
    "import gc\n",
    "import gensim\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import time\n",
    "pd.set_option('max_colwidth',400)\n",
    "from scipy.stats import spearmanr\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import scipy as sp\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from text_data import TextDataset\n",
    "from net import NeuralNet5\n",
    "from learning import Learner\n",
    "from lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# added preprocessing from https://www.kaggle.com/wowfattie/3rd-place/data\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\n', '\\xa0', '\\t',\n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"havent\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"thats\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"theres\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"theyre\":  \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "def clean_data(df, columns: list):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: clean_numbers(x))\n",
    "        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n",
    "        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path,'rb') as f:\n",
    "        emb_arr = pickle.load(f)\n",
    "    return emb_arr\n",
    "\n",
    "\n",
    "def build_matrix_adv(embedding_path: str = '',\n",
    "                 embedding_path_spellcheck: str = r'f:\\embeddings\\wiki-news-300d-1M\\wiki-news-300d-1M.vec',\n",
    "                 word_dict: dict = None, lemma_dict: dict = None, max_features: int = 100000,\n",
    "                 embed_size: int= 300, ):\n",
    "    spell_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path_spellcheck)\n",
    "    words = spell_model.index2word\n",
    "    w_rank = {}\n",
    "    for i, word in enumerate(words):\n",
    "        w_rank[word] = i\n",
    "    WORDS = w_rank\n",
    "\n",
    "    def P(word):\n",
    "        \"Probability of `word`.\"\n",
    "        # use inverse of rank as proxy\n",
    "        # returns 0 if the word isn't in the dictionary\n",
    "        return - WORDS.get(word, 0)\n",
    "\n",
    "    def correction(word):\n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(candidates(word), key=P)\n",
    "\n",
    "    def candidates(word):\n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (known([word]) or known(edits1(word)) or [word])\n",
    "\n",
    "    def known(words):\n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(w for w in words if w in WORDS)\n",
    "\n",
    "    def edits1(word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(word):\n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "    def singlify(word):\n",
    "        return \"\".join([letter for i, letter in enumerate(word) if i == 0 or letter != word[i - 1]])\n",
    "\n",
    "\n",
    "    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8'))\n",
    "\n",
    "    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore'))\n",
    "    embedding_index = load_embeddings(embedding_path)\n",
    "\n",
    "    nb_words = min(max_features, len(word_dict))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "    unknown_words = []\n",
    "    for word, i in word_dict.items():\n",
    "        key = word\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.lower())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.upper())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.capitalize())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        unknown_words.append(key)\n",
    "\n",
    "    print(f'{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings')\n",
    "    return embedding_matrix, nb_words, unknown_words\n",
    "\n",
    "\n",
    "def get_word_lemma_dict(full_text: list = None, ):\n",
    "    nlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\n",
    "    nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "    word_dict = {}\n",
    "    word_index = 1\n",
    "    lemma_dict = {}\n",
    "    docs = nlp.pipe(full_text, n_threads = os.cpu_count())\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "                word_dict[token.text] = word_index\n",
    "                word_index += 1\n",
    "                lemma_dict[token.text] = token.lemma_\n",
    "\n",
    "    return lemma_dict, word_dict\n",
    "\n",
    "\n",
    "def build_matrix(embedding_path: str = '',\n",
    "                 embedding_path_spellcheck: str = r'f:\\embeddings\\wiki-news-300d-1M\\wiki-news-300d-1M.vec',\n",
    "                 word_dict: dict = None, max_features: int = 100000,\n",
    "                 embed_size: int= 300, ):\n",
    "\n",
    "    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8'))\n",
    "    embedding_index = load_embeddings(embedding_path)\n",
    "\n",
    "    nb_words = min(max_features, len(word_dict))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "    unknown_words = []\n",
    "    for word, i in word_dict.items():\n",
    "        key = word\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.lower())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.upper())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.capitalize())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        unknown_words.append(key)\n",
    "\n",
    "    print(f'{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings')\n",
    "    return embedding_matrix, nb_words, unknown_words\n",
    "\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_features(train, test, input_columns):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/ragnar123/simple-lgbm-solution-baseline?scriptVersionId=24198335\n",
    "    \"\"\"\n",
    "    \n",
    "    # load universal sentence encoder model to get sentence ambeddings\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/4\"\n",
    "    embed = hub.load(module_url)\n",
    "    \n",
    "    # create empty dictionaries to store final results\n",
    "    embedding_train = {}\n",
    "    embedding_test = {}\n",
    "\n",
    "    # iterate over text columns to get senteces embeddings with the previous loaded model\n",
    "    for text in input_columns:\n",
    "    \n",
    "        print(text)\n",
    "        train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "        test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    \n",
    "        # create empy list to save each batch\n",
    "        curr_train_emb = []\n",
    "        curr_test_emb = []\n",
    "    \n",
    "        # define a batch to transform senteces to their correspinding embedding (1 X 512 for each sentece)\n",
    "        batch_size = 4\n",
    "        ind = 0\n",
    "        while ind * batch_size < len(train_text):\n",
    "            curr_train_emb.append(embed(train_text[ind * batch_size: (ind + 1) * batch_size])['outputs'].numpy())\n",
    "            ind += 1\n",
    "        \n",
    "        ind = 0\n",
    "        while ind * batch_size < len(test_text):\n",
    "            curr_test_emb.append(embed(test_text[ind * batch_size: (ind + 1) * batch_size])['outputs'].numpy())\n",
    "            ind += 1\n",
    "\n",
    "        # stack arrays to get a 2D array (dataframe) corresponding with all the sentences and dim 512 for columns (sentence encoder output)\n",
    "        embedding_train[text + '_embedding'] = np.vstack(curr_train_emb)\n",
    "        embedding_test[text + '_embedding'] = np.vstack(curr_test_emb)\n",
    "    \n",
    "    del embed\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    return embedding_train, embedding_test\n",
    "\n",
    "def get_dist_features(embedding_train, embedding_test):\n",
    "    \n",
    "    # define a square dist lambda function were (x1 - y1) ^ 2 + (x2 - y2) ^ 2 + (x3 - y3) ^ 2 + ... + (xn - yn) ^ 2\n",
    "    # with this we get one vector of dimension 6079\n",
    "    l2_dist = lambda x, y: np.power(x - y, 2).sum(axis = 1)\n",
    "    \n",
    "    # define a cosine dist lambda function were (x1 * y1) ^ 2 + (x2 * y2) + (x3 * y3) + ... + (xn * yn)\n",
    "    cos_dist = lambda x, y: (x * y).sum(axis = 1)\n",
    "    \n",
    "    # transpose it because we have 6 vector of dimension 6079, need 6079 x 6\n",
    "    dist_features_train = np.array([\n",
    "        l2_dist(embedding_train['question_title_embedding'], embedding_train['answer_embedding']),\n",
    "        l2_dist(embedding_train['question_body_embedding'], embedding_train['answer_embedding']),\n",
    "        l2_dist(embedding_train['question_body_embedding'], embedding_train['question_title_embedding']),\n",
    "        cos_dist(embedding_train['question_title_embedding'], embedding_train['answer_embedding']),\n",
    "        cos_dist(embedding_train['question_body_embedding'], embedding_train['answer_embedding']),\n",
    "        cos_dist(embedding_train['question_body_embedding'], embedding_train['question_title_embedding'])]).T\n",
    "    \n",
    "    # transpose it because we have 6 vector of dimension 6079, need 6079 x 6\n",
    "    dist_features_test = np.array([\n",
    "        l2_dist(embedding_test['question_title_embedding'], embedding_test['answer_embedding']),\n",
    "        l2_dist(embedding_test['question_body_embedding'], embedding_test['answer_embedding']),\n",
    "        l2_dist(embedding_test['question_body_embedding'], embedding_test['question_title_embedding']),\n",
    "        cos_dist(embedding_test['question_title_embedding'], embedding_test['answer_embedding']),\n",
    "        cos_dist(embedding_test['question_body_embedding'], embedding_test['answer_embedding']),\n",
    "        cos_dist(embedding_test['question_body_embedding'], embedding_test['question_title_embedding'])]).T\n",
    "    \n",
    "    return dist_features_train, dist_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and predicting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "from radam import RAdam\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, n_epochs=5, lr=0.001):\n",
    "    optimizer = RAdam(model.parameters(), lr)\n",
    "    patience = 4\n",
    "\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n",
    "    best_score = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "\n",
    "        for question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, y_batch in tqdm(train_loader, disable=True):\n",
    "            question = question.long().cuda()\n",
    "            answer = answer.long().cuda()\n",
    "            title = title.long().cuda()\n",
    "            category = category.long().cuda()\n",
    "            host = host.long().cuda()\n",
    "            use_emb_q = use_emb_q.cuda()\n",
    "            use_emb_a = use_emb_a.cuda()\n",
    "            use_emb_t = use_emb_t.cuda()\n",
    "            dist_feature = dist_feature.cuda()\n",
    "            \n",
    "            y_batch = y_batch.cuda()\n",
    "            y_pred = model(question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature)\n",
    "\n",
    "            loss = loss_fn(y_pred.double(), y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        avg_val_loss = 0.\n",
    "        preds = []\n",
    "        original = []\n",
    "        for i, (question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, y_batch) in enumerate(valid_loader):\n",
    "            question = question.long().cuda()\n",
    "            answer = answer.long().cuda()\n",
    "            title = title.long().cuda()\n",
    "            category = category.long().cuda()\n",
    "            host = host.long().cuda()\n",
    "            use_emb_q = use_emb_q.cuda()\n",
    "            use_emb_a = use_emb_a.cuda()\n",
    "            use_emb_t = use_emb_t.cuda()\n",
    "            dist_feature = dist_feature.cuda()\n",
    "            \n",
    "            y_batch = y_batch.cuda()\n",
    "            y_pred = model(question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature).detach()\n",
    "\n",
    "            avg_val_loss += loss_fn(y_pred.double(), y_batch).item() / len(valid_loader)\n",
    "            preds.append(y_pred.cpu().numpy())\n",
    "            original.append(y_batch.cpu().numpy())\n",
    "            \n",
    "        score = 0\n",
    "        for i in range(30):\n",
    "            score += np.nan_to_num(\n",
    "                spearmanr(np.concatenate(original)[:, i], np.concatenate(preds)[:, i]).correlation / 30)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t spearman={:.2f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, avg_loss, avg_val_loss, score, elapsed_time))\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        valid_score = score\n",
    "        if valid_score > best_score:\n",
    "            best_score = valid_score\n",
    "            p = 0\n",
    "\n",
    "        # check if validation loss didn't improve\n",
    "        if valid_score <= best_score:\n",
    "            p += 1\n",
    "            print(f'{p} epochs of non improving score')\n",
    "            if p > patience:\n",
    "                print('Stopping training')\n",
    "                stop = True\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_prediction(test_loader: DataLoader = None, model = None):\n",
    "    prediction = np.zeros((len(test_loader.dataset), 30))\n",
    "    model.eval()\n",
    "    for i, (question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, _) in enumerate(test_loader):\n",
    "\n",
    "        start_index = i * test_loader.batch_size\n",
    "        end_index   = min(start_index + test_loader.batch_size, len(test_loader.dataset))\n",
    "        question = question.long().cuda()\n",
    "        answer = answer.long().cuda()\n",
    "        title = title.long().cuda()\n",
    "        category = category.long().cuda()\n",
    "        host = host.long().cuda()\n",
    "        use_emb_q = use_emb_q.cuda()\n",
    "        use_emb_a = use_emb_a.cuda()\n",
    "        use_emb_t = use_emb_t.cuda()\n",
    "        dist_feature = dist_feature.cuda()\n",
    "        y_pred = model(question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature).detach()\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        prediction[start_index:end_index, :] +=  y_pred.detach().cpu().numpy()\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 500)\n",
    "pd.set_option('max_columns', 500)\n",
    "path = 'data/'\n",
    "sample_submission = pd.read_csv(f'{path}sample_submission.csv')\n",
    "test = pd.read_csv(f'{path}test.csv').fillna(' ')\n",
    "train = pd.read_csv(f'{path}train.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = clean_data(train, ['answer', 'question_body', 'question_title'])\n",
    "test = clean_data(test, ['answer', 'question_body', 'question_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\n",
      "question_body\n",
      "question_title\n",
      "CPU times: user 5min 54s, sys: 44.7 s, total: 6min 39s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding_train, embedding_test = get_embedding_features(train, test, ['answer', 'question_body', 'question_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.71 s, sys: 51.1 ms, total: 1.76 s\n",
      "Wall time: 177 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dist_features_train, dist_features_test  = get_dist_features(embedding_train, embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "full_text = list(train['question_body']) + \\\n",
    "                       list(train['answer']) + \\\n",
    "                       list(train['question_title']) + \\\n",
    "                       list(test['question_body']) + \\\n",
    "                       list(test['answer']) + \\\n",
    "                       list(test['question_title'])\n",
    "tokenizer.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=300\n",
    "embedding_path = \"/kaggle/input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 302 ms, total: 14.4 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemma_dict, word_dict = get_word_lemma_dict(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.5224% words are not in embeddings\n",
      "CPU times: user 3min 16s, sys: 4.02 s, total: 3min 20s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# embedding_matrix, nb_words, unknown_words = build_matrix(embedding_path, '/kaggle/input/wikinews300d1mvec/wiki-news-300d-1M.vec', tokenizer.word_index,\n",
    "#                                               100000, embed_size)\n",
    "embedding_matrix, nb_words, unknown_words = build_matrix_adv(\n",
    "    'pretrained_models/crawl-300d-2M.pkl', 'pretrained_models/wiki-news-300d-1M.vec', word_dict, lemma_dict, \n",
    "    100000, embed_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tk.word_index = {k: v for k, v in tk.word_index.items() if k in word_dict.keys()}\n",
    "# train['host'] = train['host'].apply(lambda x: x.split('.')[-2])\n",
    "# test['host'] = test['host'].apply(lambda x: x.split('.')[-2])\n",
    "unique_hosts = list(set(train['host'].unique().tolist() + test['host'].unique().tolist()))\n",
    "host_dict = {i + 1: e for i, e in enumerate(unique_hosts)}\n",
    "host_dict_reverse = {v: k for k, v in host_dict.items()}\n",
    "\n",
    "unique_categories = list(set(train['category'].unique().tolist() + test['category'].unique().tolist()))\n",
    "category_dict = {i + 1: e for i, e in enumerate(unique_categories)}\n",
    "category_dict_reverse = {v: k for k, v in category_dict.items()}\n",
    "max_len = 500\n",
    "max_len_title = 30\n",
    "train_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['question_body']), maxlen = max_len)\n",
    "train_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['answer']), maxlen = max_len)\n",
    "train_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['question_title']), maxlen = max_len_title)\n",
    "\n",
    "test_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['question_body']), maxlen = max_len)\n",
    "test_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['answer']), maxlen = max_len)\n",
    "test_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['question_title']), maxlen = max_len_title)\n",
    "\n",
    "train_host = train['host'].apply(lambda x: host_dict_reverse[x]).values\n",
    "train_category = train['category'].apply(lambda x: category_dict_reverse[x]).values\n",
    "\n",
    "test_host = test['host'].apply(lambda x: host_dict_reverse[x]).values\n",
    "test_category = test['category'].apply(lambda x: category_dict_reverse[x]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[sample_submission.columns[1:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "bs = 8\n",
    "n_cat = len(category_dict) + 1\n",
    "cat_emb = min(np.ceil((len(category_dict)) / 2), 50)\n",
    "n_host = len(host_dict)+1\n",
    "host_emb = min(np.ceil((len(host_dict)) / 2), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_test = 8\n",
    "test_loader = DataLoader(TextDataset(test_question_tokenized, test_answer_tokenized, test_title_tokenized,\n",
    "                                     test_category, test_host, embedding_test, dist_features_test, test.index),\n",
    "                          batch_size=bs_test, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Fri Nov 29 19:44:23 2019\n",
      "Epoch 1/30 \t loss=0.4244 \t val_loss=0.4140 \t spearman=0.26 \t time=101.59s\n",
      "1 epochs of non improving score\n",
      "Epoch 2/30 \t loss=0.4028 \t val_loss=0.3963 \t spearman=0.28 \t time=102.11s\n",
      "1 epochs of non improving score\n",
      "Epoch 3/30 \t loss=0.3951 \t val_loss=0.3873 \t spearman=0.31 \t time=100.42s\n",
      "1 epochs of non improving score\n",
      "Epoch 4/30 \t loss=0.3911 \t val_loss=0.3873 \t spearman=0.30 \t time=101.34s\n",
      "2 epochs of non improving score\n",
      "Epoch 5/30 \t loss=0.3903 \t val_loss=0.3853 \t spearman=0.30 \t time=103.26s\n",
      "3 epochs of non improving score\n",
      "Epoch 6/30 \t loss=0.3920 \t val_loss=0.3933 \t spearman=0.30 \t time=102.33s\n",
      "4 epochs of non improving score\n",
      "Epoch 7/30 \t loss=0.3938 \t val_loss=0.3880 \t spearman=0.30 \t time=102.34s\n",
      "5 epochs of non improving score\n",
      "Stopping training\n",
      "\n",
      "Fold 2 started at Fri Nov 29 19:56:20 2019\n",
      "Epoch 1/30 \t loss=0.4237 \t val_loss=0.4002 \t spearman=0.27 \t time=103.01s\n",
      "1 epochs of non improving score\n",
      "Epoch 2/30 \t loss=0.4005 \t val_loss=0.3899 \t spearman=0.30 \t time=101.44s\n",
      "1 epochs of non improving score\n",
      "Epoch 3/30 \t loss=0.3957 \t val_loss=0.4088 \t spearman=0.30 \t time=99.79s\n",
      "1 epochs of non improving score\n",
      "Epoch 4/30 \t loss=0.4051 \t val_loss=0.3948 \t spearman=0.29 \t time=98.33s\n",
      "2 epochs of non improving score\n",
      "Epoch 5/30 \t loss=0.3958 \t val_loss=0.3909 \t spearman=0.28 \t time=103.68s\n",
      "3 epochs of non improving score\n",
      "Epoch 6/30 \t loss=0.3992 \t val_loss=0.3943 \t spearman=0.28 \t time=104.68s\n",
      "4 epochs of non improving score\n",
      "Epoch 7/30 \t loss=0.3891 \t val_loss=0.3827 \t spearman=0.30 \t time=104.27s\n",
      "1 epochs of non improving score\n",
      "Epoch 8/30 \t loss=0.3866 \t val_loss=0.3819 \t spearman=0.31 \t time=103.26s\n",
      "1 epochs of non improving score\n",
      "Epoch 9/30 \t loss=0.3845 \t val_loss=0.3810 \t spearman=0.32 \t time=99.88s\n",
      "1 epochs of non improving score\n",
      "Epoch 10/30 \t loss=0.3838 \t val_loss=0.3797 \t spearman=0.32 \t time=100.49s\n",
      "1 epochs of non improving score\n",
      "Epoch 11/30 \t loss=0.3831 \t val_loss=0.3793 \t spearman=0.33 \t time=101.27s\n",
      "1 epochs of non improving score\n",
      "Epoch 12/30 \t loss=0.3820 \t val_loss=0.3790 \t spearman=0.32 \t time=103.96s\n",
      "2 epochs of non improving score\n",
      "Epoch 13/30 \t loss=0.3813 \t val_loss=0.3788 \t spearman=0.32 \t time=102.11s\n",
      "3 epochs of non improving score\n",
      "Epoch 14/30 \t loss=0.3801 \t val_loss=0.3781 \t spearman=0.33 \t time=101.58s\n",
      "1 epochs of non improving score\n",
      "Epoch 15/30 \t loss=0.3788 \t val_loss=0.3777 \t spearman=0.33 \t time=100.58s\n",
      "2 epochs of non improving score\n",
      "Epoch 16/30 \t loss=0.3785 \t val_loss=0.3789 \t spearman=0.32 \t time=100.44s\n",
      "3 epochs of non improving score\n",
      "Epoch 17/30 \t loss=0.3777 \t val_loss=0.3791 \t spearman=0.32 \t time=101.31s\n",
      "4 epochs of non improving score\n",
      "Epoch 18/30 \t loss=0.3774 \t val_loss=0.3794 \t spearman=0.32 \t time=102.55s\n",
      "5 epochs of non improving score\n",
      "Stopping training\n",
      "\n",
      "Fold 3 started at Fri Nov 29 20:26:57 2019\n",
      "Epoch 1/30 \t loss=0.4246 \t val_loss=0.3946 \t spearman=0.28 \t time=101.94s\n",
      "1 epochs of non improving score\n",
      "Epoch 2/30 \t loss=0.3982 \t val_loss=0.3911 \t spearman=0.30 \t time=102.02s\n",
      "1 epochs of non improving score\n",
      "Epoch 3/30 \t loss=0.3941 \t val_loss=0.3870 \t spearman=0.32 \t time=102.08s\n",
      "1 epochs of non improving score\n",
      "Epoch 4/30 \t loss=0.4013 \t val_loss=0.3889 \t spearman=0.31 \t time=100.87s\n",
      "2 epochs of non improving score\n",
      "Epoch 5/30 \t loss=0.3942 \t val_loss=0.3848 \t spearman=0.31 \t time=101.41s\n",
      "3 epochs of non improving score\n",
      "Epoch 6/30 \t loss=0.3921 \t val_loss=0.3831 \t spearman=0.32 \t time=102.81s\n",
      "1 epochs of non improving score\n",
      "Epoch 7/30 \t loss=0.3912 \t val_loss=0.3866 \t spearman=0.31 \t time=102.68s\n",
      "2 epochs of non improving score\n",
      "Epoch 8/30 \t loss=0.3913 \t val_loss=0.3814 \t spearman=0.32 \t time=105.53s\n",
      "3 epochs of non improving score\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=5, random_state=42)\n",
    "preds = np.zeros((len(test), 30))\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(train)):\n",
    "    print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "    train_loader = DataLoader(TextDataset(train_question_tokenized, train_answer_tokenized, \n",
    "                                          train_title_tokenized, train_category, train_host, embedding_train,\n",
    "                                          dist_features_train, train_index, y),\n",
    "                              batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    valid_loader = DataLoader(TextDataset(train_question_tokenized, train_answer_tokenized, \n",
    "                                          train_title_tokenized, train_category, train_host, embedding_train,\n",
    "                                          dist_features_train, valid_index, y),\n",
    "                              batch_size=bs, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "        \n",
    "    model = NeuralNet5(embedding_matrix=embedding_matrix,\n",
    "                       n_cat=n_cat,\n",
    "                       cat_emb=cat_emb,\n",
    "                       n_host=n_host,\n",
    "                       host_emb=host_emb)\n",
    "    model.cuda()\n",
    "\n",
    "    model = train_model(model, train_loader, valid_loader, n_epochs=30, lr=0.01)\n",
    "    prediction = make_prediction(test_loader, model)\n",
    "    preds += prediction / folds.n_splits\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipping is necessary or we will get an error\n",
    "sample_submission.loc[:, 'question_asker_intent_understanding':] = np.clip(preds, 0.00001, 0.999999)\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained categorical encodings visualization\n",
    "\n",
    "Just a fun visualization showing how trained categorical embeddings look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(model.children())\n",
    "g = a[3].weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=60, n_iter=1000, method='exact')\n",
    "tsne_results = tsne.fit_transform(g)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(tsne_results[:, 0], tsne_results[:, 1])\n",
    "\n",
    "for i, txt in enumerate(host_dict.values()):\n",
    "    ax.annotate(txt, (tsne_results[i, 0], tsne_results[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = a[2].weight.cpu().detach().numpy()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=60, n_iter=1000, method='exact')\n",
    "tsne_results = tsne.fit_transform(g)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(tsne_results[:, 0], tsne_results[:, 1])\n",
    "\n",
    "for i, txt in enumerate(category_dict.values()):\n",
    "    ax.annotate(txt, (tsne_results[i, 0], tsne_results[i, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}